{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LLM Security Bechmark\n",
    "This script is designed to measure the ability of LLMs to identify safe and unsafe code.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from llmScan import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84538a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the benchmark-specific template and function\n",
    "def create_benchmark_chain():\n",
    "    \"\"\"Create a chain specifically for benchmark boolean responses\"\"\"\n",
    "    model, _ = get_model_and_chain()\n",
    "    if model is None:\n",
    "        return None, None\n",
    "    \n",
    "    benchmark_template = \"\"\"<system>You are a security vulnerability analyzer for Python code.</system>\n",
    "\n",
    "<user>\n",
    "Examples of security vulnerabilities include:\n",
    "- Unsafe use of untrusted input (e.g., from `request.args`, `request.form`, environment variables, external files)\n",
    "- Dangerous function calls (e.g., `eval`, `exec`, `os.system`, `subprocess.run`, `pickle.loads`, `yaml.load`)\n",
    "- Insecure file handling (e.g., `open` or `send_file` with user-controlled paths)\n",
    "- Cryptographic mistakes (e.g., hardcoded keys, insecure algorithms)\n",
    "- Web-specific issues (e.g., Cross-Site Scripting (XSS), CSRF vulnerabilities, Open Redirects)\n",
    "- Hardcoded secrets (e.g., API keys, passwords, tokens)\n",
    "- Misconfigurations (e.g., exposing debug mode, bad CORS policies)\n",
    "\n",
    "Analyze this Python code for security vulnerabilities:\n",
    "\n",
    "```python\n",
    "{python_code}\n",
    "```\n",
    "\n",
    "Return your analysis as a single boolean value: True if vulnerable and False if safe\n",
    "```\n",
    "\n",
    "</user>\n",
    "\"\"\"\n",
    "    \n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    prompt = ChatPromptTemplate.from_template(benchmark_template)\n",
    "    chain = prompt | model\n",
    "    \n",
    "    # Get model name\n",
    "    model_name = getattr(model, 'model', 'unknown')\n",
    "    \n",
    "    return chain, model_name\n",
    "\n",
    "benchmark_chain, model_name = create_benchmark_chain()\n",
    "print(f\"Benchmark chain created successfully with model: {model_name}\" if benchmark_chain else \"Failed to create benchmark chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmark test function with timing\n",
    "def test_vulnerabilities_benchmark(python_code_path):\n",
    "    \"\"\"\n",
    "    Analyzes a Python file for security vulnerabilities using boolean response\n",
    "    \n",
    "    Args:\n",
    "        python_code_path (str): Path to the Python file to analyze\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success, result, model_name, scan_time)\n",
    "            - success (bool): True if analysis successful, False otherwise\n",
    "            - result (bool): True if vulnerable, False if safe\n",
    "            - model_name (str): Name of the LLM model used\n",
    "            - scan_time (float): Time taken for the scan in seconds\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Check if LLM is available\n",
    "        if benchmark_chain is None:\n",
    "            scan_time = time.time() - start_time\n",
    "            return False, None, \"Error: No working LLM model available\", scan_time\n",
    "        \n",
    "        # Load Python code\n",
    "        python_code = load_python_file(python_code_path)\n",
    "\n",
    "        # Invoke the chain with retries \n",
    "        retries = 0\n",
    "        while retries < MAX_RETRIES:\n",
    "            try:\n",
    "                response = benchmark_chain.invoke({\"python_code\": python_code})\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                error_msg = f\"LLM invocation failed (attempt {retries}/{MAX_RETRIES}): {str(e)}\"\n",
    "                print(error_msg)\n",
    "                if retries >= MAX_RETRIES:\n",
    "                    scan_time = time.time() - start_time\n",
    "                    return False, None, model_name, scan_time\n",
    "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "\n",
    "        # Parse response - handle both boolean and string responses\n",
    "        scan_time = time.time() - start_time\n",
    "        \n",
    "        if isinstance(response, bool):\n",
    "            return True, response, model_name, scan_time\n",
    "        elif isinstance(response, str):\n",
    "            response_lower = response.strip().lower()\n",
    "            if response_lower == \"true\":\n",
    "                return True, True, model_name, scan_time\n",
    "            elif response_lower == \"false\":\n",
    "                return True, False, model_name, scan_time\n",
    "            else:\n",
    "                # Try to extract boolean from text\n",
    "                if \"true\" in response_lower and \"false\" not in response_lower:\n",
    "                    return True, True, model_name, scan_time\n",
    "                elif \"false\" in response_lower and \"true\" not in response_lower:\n",
    "                    return True, False, model_name, scan_time\n",
    "                else:\n",
    "                    print(f\"Ambiguous response: {response}\")\n",
    "                    return False, None, model_name, scan_time\n",
    "        else:\n",
    "            print(f\"Unexpected response type: {type(response)}\")\n",
    "            return False, None, model_name, scan_time\n",
    "\n",
    "    except Exception as e:\n",
    "        scan_time = time.time() - start_time\n",
    "        return False, None, model_name, scan_time\n",
    "\n",
    "print(\"test_vulnerabilities_benchmark function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fcc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmark runner function (updated with detailed file count reporting)\n",
    "def run_benchmark(train_dir=\"train\", max_files_per_dir=None, total_max_files=None, \n",
    "                 include_patched=True, include_vulnerable=True, test_split=None):\n",
    "    \"\"\"\n",
    "    Run benchmark on Python files in train directories with flexible selection options\n",
    "    \n",
    "    Args:\n",
    "        train_dir (str): Base directory containing patched/ and vulnerable/ subdirectories\n",
    "        max_files_per_dir (int, optional): Maximum number of files to process per directory\n",
    "        total_max_files (int, optional): Maximum total number of files to process across all directories\n",
    "        include_patched (bool): Whether to include files from patched/ directory (default: True)\n",
    "        include_vulnerable (bool): Whether to include files from vulnerable/ directory (default: True)\n",
    "        test_split (float, optional): Fraction of files to use for testing (0.0-1.0). If None, use all files\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Results with columns [file_path, success, llm_model, llm_result, actual, scan_time_seconds]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_files_processed = 0\n",
    "    \n",
    "    # Track files by directory for reporting\n",
    "    files_by_directory = {}\n",
    "    \n",
    "    # Define directories and their labels based on inclusion flags\n",
    "    directories = {}\n",
    "    if include_patched:\n",
    "        directories[os.path.join(train_dir, \"patched\")] = False    # Safe files\n",
    "    if include_vulnerable:\n",
    "        directories[os.path.join(train_dir, \"vulnerable\")] = True   # Vulnerable files\n",
    "    \n",
    "    if not directories:\n",
    "        print(\"Error: No directories selected. Set include_patched=True or include_vulnerable=True\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Selected directories: {list(directories.keys())}\")\n",
    "    \n",
    "    for directory, actual_vulnerable in directories.items():\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"Warning: Directory {directory} does not exist\")\n",
    "            continue\n",
    "            \n",
    "        # Check if we've reached the total file limit\n",
    "        if total_max_files and total_files_processed >= total_max_files:\n",
    "            print(f\"Reached total file limit of {total_max_files}. Stopping.\")\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nProcessing directory: {directory}\")\n",
    "        \n",
    "        # Find all Python files in the directory\n",
    "        all_python_files = list(Path(directory).glob(\"**/*.py\"))\n",
    "        original_count = len(all_python_files)\n",
    "        print(f\"Found {original_count} Python files\")\n",
    "        \n",
    "        python_files = all_python_files.copy()\n",
    "        \n",
    "        # Apply test split if specified\n",
    "        if test_split is not None:\n",
    "            if not 0.0 <= test_split <= 1.0:\n",
    "                print(f\"Warning: test_split must be between 0.0 and 1.0, got {test_split}\")\n",
    "            else:\n",
    "                import random\n",
    "                random.seed(42)  # For reproducible results\n",
    "                random.shuffle(python_files)\n",
    "                split_size = int(len(python_files) * test_split)\n",
    "                python_files = python_files[:split_size]\n",
    "                print(f\"Using test split of {test_split:.2%}: {len(python_files)} files selected from {original_count} available\")\n",
    "        \n",
    "        # Apply per-directory limit if specified\n",
    "        if max_files_per_dir:\n",
    "            files_before_limit = len(python_files)\n",
    "            python_files = python_files[:max_files_per_dir]\n",
    "            if len(python_files) < files_before_limit:\n",
    "                print(f\"Limited to first {max_files_per_dir} files (was {files_before_limit})\")\n",
    "        \n",
    "        # Apply total limit if specified\n",
    "        if total_max_files:\n",
    "            remaining_files = total_max_files - total_files_processed\n",
    "            if remaining_files <= 0:\n",
    "                break\n",
    "            files_before_total_limit = len(python_files)\n",
    "            python_files = python_files[:remaining_files]\n",
    "            if len(python_files) < files_before_total_limit:\n",
    "                print(f\"Limited to {len(python_files)} files due to total limit (was {files_before_total_limit})\")\n",
    "        \n",
    "        # Store file count for this directory\n",
    "        directory_name = \"patched\" if not actual_vulnerable else \"vulnerable\"\n",
    "        files_by_directory[directory_name] = len(python_files)\n",
    "        \n",
    "        print(f\"Will analyze {len(python_files)} files from {directory_name} directory\")\n",
    "        \n",
    "        for file_path in python_files:\n",
    "            print(f\"Analyzing: {file_path}\")\n",
    "            \n",
    "            # Run vulnerability test with timing\n",
    "            success, llm_result, llm_model, scan_time = test_vulnerabilities_benchmark(str(file_path))\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"file_path\": str(file_path),\n",
    "                \"success\": success,\n",
    "                \"llm_model\": llm_model,\n",
    "                \"llm_result\": llm_result,\n",
    "                \"actual\": actual_vulnerable,\n",
    "                \"scan_time_seconds\": round(scan_time, 3)  # Round to 3 decimal places\n",
    "            })\n",
    "            \n",
    "            total_files_processed += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if success:\n",
    "                status = \"VULNERABLE\" if llm_result else \"SAFE\"\n",
    "                expected = \"VULNERABLE\" if actual_vulnerable else \"SAFE\"\n",
    "                match = \"✓\" if llm_result == actual_vulnerable else \"✗\"\n",
    "                print(f\"  Result: {status} | Expected: {expected} | {match} | Time: {scan_time:.3f}s\")\n",
    "            else:\n",
    "                print(f\"  Error: Analysis failed | Time: {scan_time:.3f}s\")\n",
    "            \n",
    "            # Check if we've reached the total file limit\n",
    "            if total_max_files and total_files_processed >= total_max_files:\n",
    "                print(f\"Reached total file limit of {total_max_files}. Stopping.\")\n",
    "                break\n",
    "    \n",
    "    # Print final summary of files processed by directory\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"FILES PROCESSED SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    for dir_name, count in files_by_directory.items():\n",
    "        print(f\"{dir_name.capitalize()} files: {count}\")\n",
    "    print(f\"Total files processed: {total_files_processed}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"run_benchmark function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac726af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark\n",
    "print(\"Starting LLM Security Benchmark...\")\n",
    "\n",
    "# Example usage with different options:\n",
    "\n",
    "# Basic usage:\n",
    "# df_results = run_benchmark()                           # All files from both directories\n",
    "\n",
    "# Directory selection:\n",
    "# df_results = run_benchmark(include_patched=True, include_vulnerable=False)   # Only safe files\n",
    "# df_results = run_benchmark(include_patched=False, include_vulnerable=True)   # Only vulnerable files\n",
    "\n",
    "# File limits:\n",
    "df_results = run_benchmark(max_files_per_dir=5)        # Max 5 files per directory\n",
    "# df_results = run_benchmark(total_max_files=10)         # Max 10 files total\n",
    "\n",
    "# Test split:\n",
    "# df_results = run_benchmark(test_split=0.1)             # Use 10% of files for testing\n",
    "# df_results = run_benchmark(test_split=0.5)             # Use 50% of files for testing\n",
    "\n",
    "# Combined options:\n",
    "# df_results = run_benchmark(include_vulnerable=True, include_patched=False, \n",
    "#                           test_split=0.2, max_files_per_dir=10)\n",
    "\n",
    "# Run with default settings (all files from both directories)\n",
    "# df_results = run_benchmark()\n",
    "\n",
    "print(f\"\\nBenchmark completed! Analyzed {len(df_results)} files.\")\n",
    "print(\"Results stored in 'df_results' DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf46ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display metrics (updated to include failed scans)\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic statistics\n",
    "total_files = len(df_results)\n",
    "successful_analyses = df_results['success'].sum()\n",
    "failed_analyses = total_files - successful_analyses\n",
    "\n",
    "print(f\"Total files analyzed: {total_files}\")\n",
    "print(f\"Successful analyses: {successful_analyses}\")\n",
    "print(f\"Failed analyses: {failed_analyses}\")\n",
    "\n",
    "# Show model information\n",
    "if not df_results.empty:\n",
    "    models_used = df_results['llm_model'].unique()\n",
    "    print(f\"LLM Model(s) used: {', '.join(models_used)}\")\n",
    "\n",
    "# Timing statistics\n",
    "if not df_results.empty:\n",
    "    total_time = df_results['scan_time_seconds'].sum()\n",
    "    avg_time = df_results['scan_time_seconds'].mean()\n",
    "    min_time = df_results['scan_time_seconds'].min()\n",
    "    max_time = df_results['scan_time_seconds'].max()\n",
    "    \n",
    "    print(f\"\\nTiming Statistics:\")\n",
    "    print(f\"Total scan time: {total_time:.3f} seconds\")\n",
    "    print(f\"Average scan time: {avg_time:.3f} seconds\")\n",
    "    print(f\"Fastest scan: {min_time:.3f} seconds\")\n",
    "    print(f\"Slowest scan: {max_time:.3f} seconds\")\n",
    "    \n",
    "    if successful_analyses > 0:\n",
    "        successful_df = df_results[df_results['success'] == True]\n",
    "        avg_successful_time = successful_df['scan_time_seconds'].mean()\n",
    "        print(f\"Average time for successful scans: {avg_successful_time:.3f} seconds\")\n",
    "        \n",
    "    if failed_analyses > 0:\n",
    "        failed_df = df_results[df_results['success'] == False]\n",
    "        avg_failed_time = failed_df['scan_time_seconds'].mean()\n",
    "        print(f\"Average time for failed scans: {avg_failed_time:.3f} seconds\")\n",
    "\n",
    "# Calculate metrics including failed scans\n",
    "if total_files > 0:\n",
    "    # Overall success rate\n",
    "    success_rate = successful_analyses / total_files\n",
    "    print(f\"\\nOverall Success Rate: {success_rate:.2%} ({successful_analyses}/{total_files})\")\n",
    "    \n",
    "    if successful_analyses > 0:\n",
    "        # Calculate accuracy on successful analyses only\n",
    "        successful_df = df_results[df_results['success'] == True]\n",
    "        correct_predictions = (successful_df['llm_result'] == successful_df['actual']).sum()\n",
    "        accuracy_on_successful = correct_predictions / len(successful_df)\n",
    "        \n",
    "        print(f\"Accuracy on successful scans: {accuracy_on_successful:.2%} ({correct_predictions}/{len(successful_df)})\")\n",
    "        \n",
    "        # Calculate overall accuracy (treating failed scans as incorrect)\n",
    "        overall_accuracy = correct_predictions / total_files\n",
    "        print(f\"Overall accuracy (failed scans counted as incorrect): {overall_accuracy:.2%} ({correct_predictions}/{total_files})\")\n",
    "        \n",
    "        # Confusion matrix for successful scans\n",
    "        true_positives = ((successful_df['llm_result'] == True) & (successful_df['actual'] == True)).sum()\n",
    "        true_negatives = ((successful_df['llm_result'] == False) & (successful_df['actual'] == False)).sum()\n",
    "        false_positives = ((successful_df['llm_result'] == True) & (successful_df['actual'] == False)).sum()\n",
    "        false_negatives = ((successful_df['llm_result'] == False) & (successful_df['actual'] == True)).sum()\n",
    "        \n",
    "        print(f\"\\nConfusion Matrix (Successful Scans Only):\")\n",
    "        print(f\"True Positives (correctly identified vulnerabilities): {true_positives}\")\n",
    "        print(f\"True Negatives (correctly identified safe code): {true_negatives}\")\n",
    "        print(f\"False Positives (incorrectly flagged as vulnerable): {false_positives}\")\n",
    "        print(f\"False Negatives (missed vulnerabilities): {false_negatives}\")\n",
    "        \n",
    "        # Extended confusion matrix including failed scans\n",
    "        failed_df = df_results[df_results['success'] == False]\n",
    "        failed_vulnerable = (failed_df['actual'] == True).sum()\n",
    "        failed_safe = (failed_df['actual'] == False).sum()\n",
    "        \n",
    "        print(f\"\\nExtended Confusion Matrix (Including Failed Scans):\")\n",
    "        print(f\"True Positives: {true_positives}\")\n",
    "        print(f\"True Negatives: {true_negatives}\")\n",
    "        print(f\"False Positives: {false_positives}\")\n",
    "        print(f\"False Negatives: {false_negatives}\")\n",
    "        print(f\"Failed on Vulnerable Files: {failed_vulnerable}\")\n",
    "        print(f\"Failed on Safe Files: {failed_safe}\")\n",
    "        print(f\"Total Failed: {failed_analyses}\")\n",
    "        \n",
    "        # Precision, Recall, F1 (on successful scans only)\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nMetrics (Successful Scans Only):\")\n",
    "        print(f\"Precision: {precision:.2%}\")\n",
    "        print(f\"Recall: {recall:.2%}\")\n",
    "        print(f\"F1 Score: {f1_score:.2%}\")\n",
    "        \n",
    "        # Adjusted metrics including failed scans (treating failures as false negatives/positives)\n",
    "        # For overall metrics, we'll be conservative and count failed scans as prediction errors\n",
    "        total_vulnerable = (df_results['actual'] == True).sum()\n",
    "        total_safe = (df_results['actual'] == False).sum()\n",
    "        \n",
    "        # Adjusted recall: failed vulnerable files count as missed detections\n",
    "        adjusted_recall = true_positives / total_vulnerable if total_vulnerable > 0 else 0\n",
    "        \n",
    "        # Adjusted precision: we can't count failed scans in denominator for precision\n",
    "        # so we keep the original precision calculation\n",
    "        adjusted_precision = precision\n",
    "        \n",
    "        # Adjusted F1\n",
    "        adjusted_f1 = 2 * (adjusted_precision * adjusted_recall) / (adjusted_precision + adjusted_recall) if (adjusted_precision + adjusted_recall) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nAdjusted Metrics (Failed Scans Impact Recall):\")\n",
    "        print(f\"Adjusted Precision: {adjusted_precision:.2%} (same as above)\")\n",
    "        print(f\"Adjusted Recall: {adjusted_recall:.2%} (failed vulnerable files counted as missed)\")\n",
    "        print(f\"Adjusted F1 Score: {adjusted_f1:.2%}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nNo successful analyses to calculate detailed metrics.\")\n",
    "        \n",
    "    # Failure analysis by file type\n",
    "    if failed_analyses > 0:\n",
    "        failed_df = df_results[df_results['success'] == False]\n",
    "        failed_vulnerable = (failed_df['actual'] == True).sum()\n",
    "        failed_safe = (failed_df['actual'] == False).sum()\n",
    "        \n",
    "        print(f\"\\nFailure Analysis:\")\n",
    "        print(f\"Failed vulnerable file scans: {failed_vulnerable}\")\n",
    "        print(f\"Failed safe file scans: {failed_safe}\")\n",
    "        print(f\"Failure rate on vulnerable files: {failed_vulnerable/total_vulnerable:.2%}\" if total_vulnerable > 0 else \"No vulnerable files\")\n",
    "        print(f\"Failure rate on safe files: {failed_safe/total_safe:.2%}\" if total_safe > 0 else \"No safe files\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nNo files analyzed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c38487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV (fixed variable names)\n",
    "output_file = \"llm_benchmark_results.csv\"\n",
    "df_results.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Save summary metrics to a separate file\n",
    "if successful_analyses > 0:\n",
    "    summary = {\n",
    "        \"total_files\": total_files,\n",
    "        \"successful_analyses\": successful_analyses,\n",
    "        \"failed_analyses\": failed_analyses,\n",
    "        \"llm_model\": ', '.join(models_used),\n",
    "        \"accuracy_on_successful\": accuracy_on_successful,\n",
    "        \"overall_accuracy\": overall_accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"adjusted_precision\": adjusted_precision,\n",
    "        \"adjusted_recall\": adjusted_recall,\n",
    "        \"adjusted_f1\": adjusted_f1,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"true_negatives\": true_negatives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "        \"failed_vulnerable\": failed_vulnerable,\n",
    "        \"failed_safe\": failed_safe,\n",
    "        \"total_scan_time_seconds\": total_time,\n",
    "        \"average_scan_time_seconds\": avg_time,\n",
    "        \"min_scan_time_seconds\": min_time,\n",
    "        \"max_scan_time_seconds\": max_time\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame([summary])\n",
    "    summary_df.to_csv(\"llm_benchmark_summary.csv\", index=False)\n",
    "    print(\"Summary metrics saved to: llm_benchmark_summary.csv\")\n",
    "else:\n",
    "    print(\"No successful analyses to save summary metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd33d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug LLM Input/Output\n",
    "def debug_llm_example(file_path=None):\n",
    "    \"\"\"\n",
    "    Show exactly what the LLM receives and outputs for debugging purposes\n",
    "    \n",
    "    Args:\n",
    "        file_path (str, optional): Path to a specific file to analyze. If None, uses first available file.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"LLM INPUT/OUTPUT DEBUG EXAMPLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find a file to analyze if none provided\n",
    "    if file_path is None:\n",
    "        # Look for files in train directories\n",
    "        for directory in [\"train/patched\", \"train/vulnerable\"]:\n",
    "            if os.path.exists(directory):\n",
    "                python_files = list(Path(directory).glob(\"**/*.py\"))\n",
    "                if python_files:\n",
    "                    file_path = str(python_files[0])\n",
    "                    break\n",
    "        \n",
    "        if file_path is None:\n",
    "            print(\"No Python files found in train/patched or train/vulnerable directories\")\n",
    "            return\n",
    "    \n",
    "    print(f\"Analyzing file: {file_path}\")\n",
    "    print(f\"File type: {'VULNERABLE' if 'vulnerable' in file_path else 'SAFE'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Load the Python code\n",
    "        python_code = load_python_file(file_path)\n",
    "        \n",
    "        print(\"PYTHON CODE CONTENT:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(python_code)\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Show the prompt template\n",
    "        benchmark_template = \"\"\"<system>You are a security vulnerability analyzer for Python code.</system>\n",
    "\n",
    "<user>\n",
    "Examples of security vulnerabilities include:\n",
    "- Unsafe use of untrusted input (e.g., from `request.args`, `request.form`, environment variables, external files)\n",
    "- Dangerous function calls (e.g., `eval`, `exec`, `os.system`, `subprocess.run`, `pickle.loads`, `yaml.load`)\n",
    "- Insecure file handling (e.g., `open` or `send_file` with user-controlled paths)\n",
    "- Cryptographic mistakes (e.g., hardcoded keys, insecure algorithms)\n",
    "- Web-specific issues (e.g., Cross-Site Scripting (XSS), CSRF vulnerabilities, Open Redirects)\n",
    "- Hardcoded secrets (e.g., API keys, passwords, tokens)\n",
    "- Misconfigurations (e.g., exposing debug mode, bad CORS policies)\n",
    "\n",
    "Analyze this Python code for security vulnerabilities:\n",
    "\n",
    "```python\n",
    "{python_code}\n",
    "```\n",
    "\n",
    "Return your analysis as a single boolean value: True if vulnerable and False if safe\n",
    "```\n",
    "\n",
    "</user>\n",
    "\"\"\"\n",
    "        \n",
    "        # Format the full prompt\n",
    "        full_prompt = benchmark_template.format(python_code=python_code)\n",
    "        \n",
    "        print(\"FULL PROMPT SENT TO LLM:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(full_prompt)\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Get LLM response\n",
    "        if benchmark_chain is None:\n",
    "            print(\"ERROR: No LLM chain available\")\n",
    "            return\n",
    "            \n",
    "        print(\"Sending to LLM...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = benchmark_chain.invoke({\"python_code\": python_code})\n",
    "            scan_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"LLM RESPONSE (took {scan_time:.3f}s):\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Raw response: {repr(response)}\")\n",
    "            print(f\"Response type: {type(response)}\")\n",
    "            print(f\"Response content: {response}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Show how the response gets parsed\n",
    "            if isinstance(response, bool):\n",
    "                parsed_result = response\n",
    "                print(f\"PARSED RESULT: {parsed_result} (direct boolean)\")\n",
    "            elif isinstance(response, str):\n",
    "                response_lower = response.strip().lower()\n",
    "                if response_lower == \"true\":\n",
    "                    parsed_result = True\n",
    "                    print(f\"PARSED RESULT: {parsed_result} (string 'true')\")\n",
    "                elif response_lower == \"false\":\n",
    "                    parsed_result = False\n",
    "                    print(f\"PARSED RESULT: {parsed_result} (string 'false')\")\n",
    "                elif \"true\" in response_lower and \"false\" not in response_lower:\n",
    "                    parsed_result = True\n",
    "                    print(f\"PARSED RESULT: {parsed_result} (contains 'true')\")\n",
    "                elif \"false\" in response_lower and \"true\" not in response_lower:\n",
    "                    parsed_result = False\n",
    "                    print(f\"PARSED RESULT: {parsed_result} (contains 'false')\")\n",
    "                else:\n",
    "                    parsed_result = None\n",
    "                    print(f\"PARSED RESULT: {parsed_result} (ambiguous response)\")\n",
    "            else:\n",
    "                parsed_result = None\n",
    "                print(f\"PARSED RESULT: {parsed_result} (unexpected type)\")\n",
    "            \n",
    "            # Show expected vs actual\n",
    "            expected_result = \"vulnerable\" in file_path.lower()\n",
    "            print(f\"EXPECTED RESULT: {expected_result}\")\n",
    "            \n",
    "            if parsed_result is not None:\n",
    "                match = \"✓ CORRECT\" if parsed_result == expected_result else \"✗ INCORRECT\"\n",
    "                print(f\"MATCH: {match}\")\n",
    "            else:\n",
    "                print(\"MATCH: ✗ FAILED TO PARSE\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"LLM ERROR: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"FILE LOADING ERROR: {str(e)}\")\n",
    "\n",
    "# Run the debug example\n",
    "print(\"Running LLM debug example...\")\n",
    "debug_llm_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
