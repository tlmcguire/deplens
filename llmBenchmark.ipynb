{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LLM Security Bechmark\n",
    "This script is designed to measure the ability of LLMs to identify safe and unsafe code.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from llmScan import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84538a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the benchmark-specific template and function\n",
    "def create_benchmark_chain():\n",
    "    \"\"\"Create a chain specifically for benchmark boolean responses\"\"\"\n",
    "    model, _ = get_model_and_chain()\n",
    "    if model is None:\n",
    "        return None, None\n",
    "    \n",
    "    benchmark_template = \"\"\"<system>You are a security vulnerability analyzer for Python code.</system>\n",
    "\n",
    "<user>\n",
    "Examples of security vulnerabilities include:\n",
    "- Unsafe use of untrusted input (e.g., from `request.args`, `request.form`, environment variables, external files)\n",
    "- Dangerous function calls (e.g., `eval`, `exec`, `os.system`, `subprocess.run`, `pickle.loads`, `yaml.load`)\n",
    "- Insecure file handling (e.g., `open` or `send_file` with user-controlled paths)\n",
    "- Cryptographic mistakes (e.g., hardcoded keys, insecure algorithms)\n",
    "- Web-specific issues (e.g., Cross-Site Scripting (XSS), CSRF vulnerabilities, Open Redirects)\n",
    "- Hardcoded secrets (e.g., API keys, passwords, tokens)\n",
    "- Misconfigurations (e.g., exposing debug mode, bad CORS policies)\n",
    "\n",
    "Analyze this Python code for security vulnerabilities:\n",
    "\n",
    "```python\n",
    "{python_code}\n",
    "```\n",
    "\n",
    "Return your analysis as a single boolean value: True if vulnerable and False if safe\n",
    "```\n",
    "\n",
    "</user>\n",
    "\"\"\"\n",
    "    \n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    prompt = ChatPromptTemplate.from_template(benchmark_template)\n",
    "    chain = prompt | model\n",
    "    \n",
    "    # Get model name\n",
    "    model_name = getattr(model, 'model', 'unknown')\n",
    "    \n",
    "    return chain, model_name\n",
    "\n",
    "benchmark_chain, model_name = create_benchmark_chain()\n",
    "print(f\"Benchmark chain created successfully with model: {model_name}\" if benchmark_chain else \"Failed to create benchmark chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmark test function\n",
    "def test_vulnerabilities_benchmark(python_code_path):\n",
    "    \"\"\"\n",
    "    Analyzes a Python file for security vulnerabilities using boolean response\n",
    "    \n",
    "    Args:\n",
    "        python_code_path (str): Path to the Python file to analyze\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success, result, model_name)\n",
    "            - success (bool): True if analysis successful, False otherwise\n",
    "            - result (bool): True if vulnerable, False if safe\n",
    "            - model_name (str): Name of the LLM model used\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if LLM is available\n",
    "        if benchmark_chain is None:\n",
    "            return False, None, \"Error: No working LLM model available\"\n",
    "        \n",
    "        # Load Python code\n",
    "        python_code = load_python_file(python_code_path)\n",
    "\n",
    "        # Invoke the chain with retries \n",
    "        retries = 0\n",
    "        while retries < MAX_RETRIES:\n",
    "            try:\n",
    "                response = benchmark_chain.invoke({\"python_code\": python_code})\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                error_msg = f\"LLM invocation failed (attempt {retries}/{MAX_RETRIES}): {str(e)}\"\n",
    "                print(error_msg)\n",
    "                if retries >= MAX_RETRIES:\n",
    "                    return False, None, model_name\n",
    "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "\n",
    "        # Parse response - handle both boolean and string responses\n",
    "        if isinstance(response, bool):\n",
    "            return True, response, model_name\n",
    "        elif isinstance(response, str):\n",
    "            response_lower = response.strip().lower()\n",
    "            if response_lower == \"true\":\n",
    "                return True, True, model_name\n",
    "            elif response_lower == \"false\":\n",
    "                return True, False, model_name\n",
    "            else:\n",
    "                # Try to extract boolean from text\n",
    "                if \"true\" in response_lower and \"false\" not in response_lower:\n",
    "                    return True, True, model_name\n",
    "                elif \"false\" in response_lower and \"true\" not in response_lower:\n",
    "                    return True, False, model_name\n",
    "                else:\n",
    "                    print(f\"Ambiguous response: {response}\")\n",
    "                    return False, None, model_name\n",
    "        else:\n",
    "            print(f\"Unexpected response type: {type(response)}\")\n",
    "            return False, None, model_name\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, None, model_name\n",
    "\n",
    "print(\"test_vulnerabilities_benchmark function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fcc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmark test function\n",
    "def test_vulnerabilities_benchmark(python_code_path):\n",
    "    \"\"\"\n",
    "    Analyzes a Python file for security vulnerabilities using boolean response\n",
    "    \n",
    "    Args:\n",
    "        python_code_path (str): Path to the Python file to analyze\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success, result, model_name)\n",
    "            - success (bool): True if analysis successful, False otherwise\n",
    "            - result (bool): True if vulnerable, False if safe\n",
    "            - model_name (str): Name of the LLM model used\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if LLM is available\n",
    "        if benchmark_chain is None:\n",
    "            return False, None, \"Error: No working LLM model available\"\n",
    "        \n",
    "        # Load Python code\n",
    "        python_code = load_python_file(python_code_path)\n",
    "\n",
    "        # Invoke the chain with retries \n",
    "        retries = 0\n",
    "        while retries < MAX_RETRIES:\n",
    "            try:\n",
    "                response = benchmark_chain.invoke({\"python_code\": python_code})\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                error_msg = f\"LLM invocation failed (attempt {retries}/{MAX_RETRIES}): {str(e)}\"\n",
    "                print(error_msg)\n",
    "                if retries >= MAX_RETRIES:\n",
    "                    return False, None, model_name\n",
    "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "\n",
    "        # Parse response - handle both boolean and string responses\n",
    "        if isinstance(response, bool):\n",
    "            return True, response, model_name\n",
    "        elif isinstance(response, str):\n",
    "            response_lower = response.strip().lower()\n",
    "            if response_lower == \"true\":\n",
    "                return True, True, model_name\n",
    "            elif response_lower == \"false\":\n",
    "                return True, False, model_name\n",
    "            else:\n",
    "                # Try to extract boolean from text\n",
    "                if \"true\" in response_lower and \"false\" not in response_lower:\n",
    "                    return True, True, model_name\n",
    "                elif \"false\" in response_lower and \"true\" not in response_lower:\n",
    "                    return True, False, model_name\n",
    "                else:\n",
    "                    print(f\"Ambiguous response: {response}\")\n",
    "                    return False, None, model_name\n",
    "        else:\n",
    "            print(f\"Unexpected response type: {type(response)}\")\n",
    "            return False, None, model_name\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, None, model_name\n",
    "\n",
    "print(\"test_vulnerabilities_benchmark function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45264e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmark runner function\n",
    "def run_benchmark(train_dir=\"train\"):\n",
    "    \"\"\"\n",
    "    Run benchmark on all Python files in train/patched/ and train/vulnerable/ directories\n",
    "    \n",
    "    Args:\n",
    "        train_dir (str): Base directory containing patched/ and vulnerable/ subdirectories\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Results with columns [file_path, success, llm_model, llm_result, actual]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Define directories and their labels\n",
    "    directories = {\n",
    "        os.path.join(train_dir, \"patched\"): False,    # Safe files\n",
    "        os.path.join(train_dir, \"vulnerable\"): True   # Vulnerable files\n",
    "    }\n",
    "    \n",
    "    for directory, actual_vulnerable in directories.items():\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"Warning: Directory {directory} does not exist\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing directory: {directory}\")\n",
    "        \n",
    "        # Find all Python files in the directory\n",
    "        python_files = list(Path(directory).glob(\"**/*.py\"))\n",
    "        print(f\"Found {len(python_files)} Python files\")\n",
    "        \n",
    "        for file_path in python_files:\n",
    "            print(f\"Analyzing: {file_path}\")\n",
    "            \n",
    "            # Run vulnerability test\n",
    "            success, llm_result, llm_model = test_vulnerabilities_benchmark(str(file_path))\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"file_path\": str(file_path),\n",
    "                \"success\": success,\n",
    "                \"llm_model\": llm_model,\n",
    "                \"llm_result\": llm_result,\n",
    "                \"actual\": actual_vulnerable\n",
    "            })\n",
    "            \n",
    "            # Print progress\n",
    "            if success:\n",
    "                status = \"VULNERABLE\" if llm_result else \"SAFE\"\n",
    "                expected = \"VULNERABLE\" if actual_vulnerable else \"SAFE\"\n",
    "                match = \"✓\" if llm_result == actual_vulnerable else \"✗\"\n",
    "                print(f\"  Result: {status} | Expected: {expected} | {match}\")\n",
    "            else:\n",
    "                print(f\"  Error: Analysis failed\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"run_benchmark function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac726af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark\n",
    "print(\"Starting LLM Security Benchmark...\")\n",
    "df_results = run_benchmark()\n",
    "\n",
    "print(f\"\\nBenchmark completed! Analyzed {len(df_results)} files.\")\n",
    "print(\"Results stored in 'df_results' DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf46ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display metrics\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic statistics\n",
    "total_files = len(df_results)\n",
    "successful_analyses = df_results['success'].sum()\n",
    "failed_analyses = total_files - successful_analyses\n",
    "\n",
    "print(f\"Total files analyzed: {total_files}\")\n",
    "print(f\"Successful analyses: {successful_analyses}\")\n",
    "print(f\"Failed analyses: {failed_analyses}\")\n",
    "\n",
    "# Show model information\n",
    "if not df_results.empty:\n",
    "    models_used = df_results['llm_model'].unique()\n",
    "    print(f\"LLM Model(s) used: {', '.join(models_used)}\")\n",
    "\n",
    "if successful_analyses > 0:\n",
    "    # Calculate accuracy on successful analyses only\n",
    "    successful_df = df_results[df_results['success'] == True]\n",
    "    correct_predictions = (successful_df['llm_result'] == successful_df['actual']).sum()\n",
    "    accuracy = correct_predictions / len(successful_df)\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy:.2%} ({correct_predictions}/{len(successful_df)})\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    true_positives = ((successful_df['llm_result'] == True) & (successful_df['actual'] == True)).sum()\n",
    "    true_negatives = ((successful_df['llm_result'] == False) & (successful_df['actual'] == False)).sum()\n",
    "    false_positives = ((successful_df['llm_result'] == True) & (successful_df['actual'] == False)).sum()\n",
    "    false_negatives = ((successful_df['llm_result'] == False) & (successful_df['actual'] == True)).sum()\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives (correctly identified vulnerabilities): {true_positives}\")\n",
    "    print(f\"True Negatives (correctly identified safe code): {true_negatives}\")\n",
    "    print(f\"False Positives (incorrectly flagged as vulnerable): {false_positives}\")\n",
    "    print(f\"False Negatives (missed vulnerabilities): {false_negatives}\")\n",
    "    \n",
    "    # Precision, Recall, F1\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"Precision: {precision:.2%}\")\n",
    "    print(f\"Recall: {recall:.2%}\")\n",
    "    print(f\"F1 Score: {f1_score:.2%}\")\n",
    "else:\n",
    "    print(\"\\nNo successful analyses to calculate metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c38487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "output_file = \"llm_benchmark_results.csv\"\n",
    "df_results.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Save summary metrics to a separate file\n",
    "if successful_analyses > 0:\n",
    "    summary = {\n",
    "        \"total_files\": total_files,\n",
    "        \"successful_analyses\": successful_analyses,\n",
    "        \"failed_analyses\": failed_analyses,\n",
    "        \"llm_model\": ', '.join(models_used),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"true_negatives\": true_negatives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame([summary])\n",
    "    summary_df.to_csv(\"llm_benchmark_summary.csv\", index=False)\n",
    "    print(\"Summary metrics saved to: llm_benchmark_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
