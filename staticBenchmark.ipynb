{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Static Security Bechmark\n",
    "This script is designed to measure the ability of static analysis tools to identify safe and unsafe code.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from llmScan import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0063ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmark test function with timing\n",
    "\n",
    "def test_vulnerabilities_benchmark(python_code_path):\n",
    "    \"\"\"\n",
    "    Analyzes a Python file for security vulnerabilities using Bandit (plain text output).\n",
    "\n",
    "    Args:\n",
    "        python_code_path (str): Path to the Python file to analyze\n",
    "\n",
    "    Returns:\n",
    "        tuple: (success, result, tool_name, scan_time)\n",
    "            - success (bool): True if analysis successful, False otherwise\n",
    "            - result (bool): True if vulnerable, False if safe\n",
    "            - tool_name (str): Name of the static tool used\n",
    "            - scan_time (float): Time taken for the scan in seconds\n",
    "    \"\"\"\n",
    "    import re\n",
    "    start_time = time.time()\n",
    "    tool_name = \"Bandit\"\n",
    "    command = [\n",
    "        'bandit',\n",
    "        '-r', python_code_path,\n",
    "        '-ll', '-iii'\n",
    "    ]\n",
    "    try:\n",
    "        proc = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        output = proc.stdout\n",
    "        \n",
    "        # Store raw output for debugging\n",
    "        global last_bandit_output\n",
    "        last_bandit_output = output\n",
    "        \n",
    "        # Look for signs of vulnerability\n",
    "        if \">> Issue:\" in output:\n",
    "            is_vulnerable = True\n",
    "        elif \"No issues identified.\" in output:\n",
    "            is_vulnerable = False\n",
    "        elif \"Files skipped\" in output and \"No such file or directory\" in output:\n",
    "            print(f\"File not found: {python_code_path}\")\n",
    "            scan_time = time.time() - start_time\n",
    "            return False, None, tool_name, scan_time\n",
    "        else:\n",
    "            # Could not determine, treat as error\n",
    "            print(f\"Bandit output ambiguous for file: {python_code_path}\")\n",
    "            scan_time = time.time() - start_time\n",
    "            return False, None, tool_name, scan_time\n",
    "\n",
    "        scan_time = time.time() - start_time\n",
    "        return True, is_vulnerable, tool_name, scan_time\n",
    "    except Exception as e:\n",
    "        print(f\"Bandit error: {e}\")\n",
    "        scan_time = time.time() - start_time\n",
    "        return False, None, tool_name, scan_time\n",
    "\n",
    "# Initialize a global variable to store the last output\n",
    "last_bandit_output = \"\"\n",
    "\n",
    "print(\"test_vulnerabilities_benchmark function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cbc21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmark runner function\n",
    "def run_benchmark(train_dir=\"train\", max_files_per_dir=None, total_max_files=None,\n",
    "                 include_patched=True, include_vulnerable=True, test_split=None):\n",
    "    \"\"\"\n",
    "    Run Bandit benchmark on Python files in train directories.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Results with columns [file_path, success, tool_name, bandit_result, actual, scan_time_seconds]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_files_processed = 0\n",
    "    files_by_directory = {}\n",
    "\n",
    "    directories = {}\n",
    "    if include_patched:\n",
    "        directories[os.path.join(train_dir, \"patched\")] = False\n",
    "    if include_vulnerable:\n",
    "        directories[os.path.join(train_dir, \"vulnerable\")] = True\n",
    "\n",
    "    if not directories:\n",
    "        print(\"Error: No directories selected.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Selected directories: {list(directories.keys())}\")\n",
    "\n",
    "    for directory, actual_vulnerable in directories.items():\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"Warning: Directory {directory} does not exist\")\n",
    "            continue\n",
    "\n",
    "        if total_max_files and total_files_processed >= total_max_files:\n",
    "            print(f\"Reached total file limit of {total_max_files}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        print(f\"\\nProcessing directory: {directory}\")\n",
    "        all_python_files = list(Path(directory).glob(\"**/*.py\"))\n",
    "        original_count = len(all_python_files)\n",
    "        print(f\"Found {original_count} Python files\")\n",
    "\n",
    "        python_files = all_python_files.copy()\n",
    "\n",
    "        # Test split\n",
    "        if test_split is not None and 0.0 <= test_split <= 1.0:\n",
    "            import random\n",
    "            random.seed(42)\n",
    "            random.shuffle(python_files)\n",
    "            split_size = int(len(python_files) * test_split)\n",
    "            python_files = python_files[:split_size]\n",
    "            print(f\"Using test split of {test_split:.2%}: {len(python_files)} files selected from {original_count}\")\n",
    "\n",
    "        # Per-directory limit\n",
    "        if max_files_per_dir:\n",
    "            python_files = python_files[:max_files_per_dir]\n",
    "\n",
    "        # Total limit\n",
    "        if total_max_files:\n",
    "            remaining_files = total_max_files - total_files_processed\n",
    "            python_files = python_files[:remaining_files]\n",
    "\n",
    "        directory_name = \"patched\" if not actual_vulnerable else \"vulnerable\"\n",
    "        files_by_directory[directory_name] = len(python_files)\n",
    "        print(f\"Will analyze {len(python_files)} files from {directory_name} directory\")\n",
    "\n",
    "        for file_path in python_files:\n",
    "            print(f\"Analyzing: {file_path}\")\n",
    "            success, bandit_result, tool_name, scan_time = test_vulnerabilities_benchmark(str(file_path))\n",
    "            results.append({\n",
    "                \"file_path\": str(file_path),\n",
    "                \"success\": success,\n",
    "                \"tool_name\": tool_name,\n",
    "                \"bandit_result\": bandit_result,\n",
    "                \"actual\": actual_vulnerable,\n",
    "                \"scan_time_seconds\": round(scan_time, 3)\n",
    "            })\n",
    "            total_files_processed += 1\n",
    "\n",
    "            if success:\n",
    "                status = \"VULNERABLE\" if bandit_result else \"SAFE\"\n",
    "                expected = \"VULNERABLE\" if actual_vulnerable else \"SAFE\"\n",
    "                match = \"✓\" if bandit_result == actual_vulnerable else \"✗\"\n",
    "                print(f\"  Result: {status} | Expected: {expected} | {match} | Time: {scan_time:.3f}s\")\n",
    "            else:\n",
    "                print(f\"  Error: Analysis failed | Time: {scan_time:.3f}s\")\n",
    "\n",
    "            if total_max_files and total_files_processed >= total_max_files:\n",
    "                print(f\"Reached total file limit of {total_max_files}. Stopping.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\n{'='*50}\\nFILES PROCESSED SUMMARY\\n{'='*50}\")\n",
    "    for dir_name, count in files_by_directory.items():\n",
    "        print(f\"{dir_name.capitalize()} files: {count}\")\n",
    "    print(f\"Total files processed: {total_files_processed}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"run_benchmark function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229abd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark\n",
    "print(\"Starting Bandit Security Benchmark...\")\n",
    "\n",
    "# Example usage with different options:\n",
    "\n",
    "# Basic usage:\n",
    "# df_results = run_benchmark()                           # All files from both directories\n",
    "\n",
    "# Directory selection:\n",
    "# df_results = run_benchmark(include_patched=True, include_vulnerable=False)   # Only safe files\n",
    "# df_results = run_benchmark(include_patched=False, include_vulnerable=True)   # Only vulnerable files\n",
    "\n",
    "# File limits:\n",
    "df_results = run_benchmark(max_files_per_dir=50)        # Max 50 files per directory\n",
    "# df_results = run_benchmark(total_max_files=10)         # Max 10 files total\n",
    "\n",
    "# Test split:\n",
    "# df_results = run_benchmark(test_split=0.1)             # Use 10% of files for testing\n",
    "# df_results = run_benchmark(test_split=0.5)             # Use 50% of files for testing\n",
    "\n",
    "# Combined options:\n",
    "# df_results = run_benchmark(include_vulnerable=True, include_patched=False, \n",
    "#                            test_split=0.2, max_files_per_dir=10)\n",
    "\n",
    "# Run with default settings (all files from both directories)\n",
    "# df_results = run_benchmark()\n",
    "\n",
    "print(f\"\\nBenchmark completed! Analyzed {len(df_results)} files.\")\n",
    "print(\"Results stored in 'df_results' DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dab6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display metrics\n",
    "print(\"=\"*60)\n",
    "print(\"BANDIT BENCHMARK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_files = len(df_results)\n",
    "successful_analyses = df_results['success'].sum()\n",
    "failed_analyses = total_files - successful_analyses\n",
    "\n",
    "print(f\"Total files analyzed: {total_files}\")\n",
    "print(f\"Successful analyses: {successful_analyses}\")\n",
    "print(f\"Failed analyses: {failed_analyses}\")\n",
    "\n",
    "if not df_results.empty:\n",
    "    tools_used = df_results['tool_name'].unique()\n",
    "    print(f\"Static Tool(s) used: {', '.join(tools_used)}\")\n",
    "\n",
    "    total_time = df_results['scan_time_seconds'].sum()\n",
    "    avg_time = df_results['scan_time_seconds'].mean()\n",
    "    min_time = df_results['scan_time_seconds'].min()\n",
    "    max_time = df_results['scan_time_seconds'].max()\n",
    "\n",
    "    print(f\"\\nTiming Statistics:\")\n",
    "    print(f\"Total scan time: {total_time:.3f} seconds\")\n",
    "    print(f\"Average scan time: {avg_time:.3f} seconds\")\n",
    "    print(f\"Fastest scan: {min_time:.3f} seconds\")\n",
    "    print(f\"Slowest scan: {max_time:.3f} seconds\")\n",
    "\n",
    "    if successful_analyses > 0:\n",
    "        successful_df = df_results[df_results['success'] == True]\n",
    "        avg_successful_time = successful_df['scan_time_seconds'].mean()\n",
    "        print(f\"Average time for successful scans: {avg_successful_time:.3f} seconds\")\n",
    "\n",
    "    if failed_analyses > 0:\n",
    "        failed_df = df_results[df_results['success'] == False]\n",
    "        avg_failed_time = failed_df['scan_time_seconds'].mean()\n",
    "        print(f\"Average time for failed scans: {avg_failed_time:.3f} seconds\")\n",
    "\n",
    "if total_files > 0:\n",
    "    success_rate = successful_analyses / total_files\n",
    "    print(f\"\\nOverall Success Rate: {success_rate:.2%} ({successful_analyses}/{total_files})\")\n",
    "\n",
    "    if successful_analyses > 0:\n",
    "        successful_df = df_results[df_results['success'] == True]\n",
    "        correct_predictions = (successful_df['bandit_result'] == successful_df['actual']).sum()\n",
    "        accuracy_on_successful = correct_predictions / len(successful_df)\n",
    "        overall_accuracy = correct_predictions / total_files\n",
    "\n",
    "        print(f\"Accuracy on successful scans: {accuracy_on_successful:.2%} ({correct_predictions}/{len(successful_df)})\")\n",
    "        print(f\"Overall accuracy (failed scans counted as incorrect): {overall_accuracy:.2%} ({correct_predictions}/{total_files})\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        true_positives = ((successful_df['bandit_result'] == True) & (successful_df['actual'] == True)).sum()\n",
    "        true_negatives = ((successful_df['bandit_result'] == False) & (successful_df['actual'] == False)).sum()\n",
    "        false_positives = ((successful_df['bandit_result'] == True) & (successful_df['actual'] == False)).sum()\n",
    "        false_negatives = ((successful_df['bandit_result'] == False) & (successful_df['actual'] == True)).sum()\n",
    "\n",
    "        print(f\"\\nConfusion Matrix (Successful Scans Only):\")\n",
    "        print(f\"True Positives: {true_positives}\")\n",
    "        print(f\"True Negatives: {true_negatives}\")\n",
    "        print(f\"False Positives: {false_positives}\")\n",
    "        print(f\"False Negatives: {false_negatives}\")\n",
    "\n",
    "        # Extended confusion matrix\n",
    "        failed_df = df_results[df_results['success'] == False]\n",
    "        failed_vulnerable = (failed_df['actual'] == True).sum()\n",
    "        failed_safe = (failed_df['actual'] == False).sum()\n",
    "\n",
    "        print(f\"\\nExtended Confusion Matrix (Including Failed Scans):\")\n",
    "        print(f\"True Positives: {true_positives}\")\n",
    "        print(f\"True Negatives: {true_negatives}\")\n",
    "        print(f\"False Positives: {false_positives}\")\n",
    "        print(f\"False Negatives: {false_negatives}\")\n",
    "        print(f\"Failed on Vulnerable Files: {failed_vulnerable}\")\n",
    "        print(f\"Failed on Safe Files: {failed_safe}\")\n",
    "        print(f\"Total Failed: {failed_analyses}\")\n",
    "\n",
    "        # Precision, Recall, F1 (on successful scans only)\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        print(f\"\\nMetrics (Successful Scans Only):\")\n",
    "        print(f\"Precision: {precision:.2%}\")\n",
    "        print(f\"Recall: {recall:.2%}\")\n",
    "        print(f\"F1 Score: {f1_score:.2%}\")\n",
    "\n",
    "        # Adjusted metrics including failed scans\n",
    "        total_vulnerable = (df_results['actual'] == True).sum()\n",
    "        total_safe = (df_results['actual'] == False).sum()\n",
    "        adjusted_recall = true_positives / total_vulnerable if total_vulnerable > 0 else 0\n",
    "        adjusted_precision = precision\n",
    "        adjusted_f1 = 2 * (adjusted_precision * adjusted_recall) / (adjusted_precision + adjusted_recall) if (adjusted_precision + adjusted_recall) > 0 else 0\n",
    "\n",
    "        print(f\"\\nAdjusted Metrics (Failed Scans Impact Recall):\")\n",
    "        print(f\"Adjusted Precision: {adjusted_precision:.2%} (same as above)\")\n",
    "        print(f\"Adjusted Recall: {adjusted_recall:.2%} (failed vulnerable files counted as missed)\")\n",
    "        print(f\"Adjusted F1 Score: {adjusted_f1:.2%}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo successful analyses to calculate detailed metrics.\")\n",
    "\n",
    "    # Failure analysis by file type\n",
    "    if failed_analyses > 0:\n",
    "        failed_df = df_results[df_results['success'] == False]\n",
    "        failed_vulnerable = (failed_df['actual'] == True).sum()\n",
    "        failed_safe = (failed_df['actual'] == False).sum()\n",
    "\n",
    "        print(f\"\\nFailure Analysis:\")\n",
    "        print(f\"Failed vulnerable file scans: {failed_vulnerable}\")\n",
    "        print(f\"Failed safe file scans: {failed_safe}\")\n",
    "        print(f\"Failure rate on vulnerable files: {failed_vulnerable/total_vulnerable:.2%}\" if total_vulnerable > 0 else \"No vulnerable files\")\n",
    "        print(f\"Failure rate on safe files: {failed_safe/total_safe:.2%}\" if total_safe > 0 else \"No safe files\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo files analyzed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1009b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "output_file = \"bandit_benchmark_results.csv\"\n",
    "df_results.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Save summary metrics to a separate file\n",
    "if successful_analyses > 0:\n",
    "    summary = {\n",
    "        \"total_files\": total_files,\n",
    "        \"successful_analyses\": successful_analyses,\n",
    "        \"failed_analyses\": failed_analyses,\n",
    "        \"tool_name\": ', '.join(tools_used),\n",
    "        \"accuracy_on_successful\": accuracy_on_successful,\n",
    "        \"overall_accuracy\": overall_accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"adjusted_precision\": adjusted_precision,\n",
    "        \"adjusted_recall\": adjusted_recall,\n",
    "        \"adjusted_f1\": adjusted_f1,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"true_negatives\": true_negatives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "        \"failed_vulnerable\": failed_vulnerable,\n",
    "        \"failed_safe\": failed_safe,\n",
    "        \"total_scan_time_seconds\": total_time,\n",
    "        \"average_scan_time_seconds\": avg_time,\n",
    "        \"min_scan_time_seconds\": min_time,\n",
    "        \"max_scan_time_seconds\": max_time\n",
    "    }\n",
    "    summary_df = pd.DataFrame([summary])\n",
    "    summary_df.to_csv(\"bandit_benchmark_summary.csv\", index=False)\n",
    "    print(\"Summary metrics saved to: bandit_benchmark_summary.csv\")\n",
    "else:\n",
    "    print(\"No successful analyses to save summary metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug Bandit Input/Output\n",
    "def debug_bandit_example(file_path=None):\n",
    "    \"\"\"\n",
    "    Show exactly what Bandit receives and outputs for debugging purposes\n",
    "\n",
    "    Args:\n",
    "        file_path (str, optional): Path to a specific file to analyze. If None, uses first available file.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"BANDIT INPUT/OUTPUT DEBUG EXAMPLE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Find a file to analyze if none provided\n",
    "    if file_path is None:\n",
    "        for directory in [\"train/patched\", \"train/vulnerable\"]:\n",
    "            if os.path.exists(directory):\n",
    "                python_files = list(Path(directory).glob(\"**/*.py\"))\n",
    "                if python_files:\n",
    "                    file_path = str(python_files[0])\n",
    "                    break\n",
    "        if file_path is None:\n",
    "            print(\"No Python files found in train/patched or train/vulnerable directories\")\n",
    "            return\n",
    "\n",
    "    print(f\"Analyzing file: {file_path}\")\n",
    "    print(f\"File type: {'VULNERABLE' if 'vulnerable' in file_path else 'SAFE'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            python_code = f.read()\n",
    "        print(\"PYTHON CODE CONTENT:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(python_code)\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        print(\"Running Bandit...\")\n",
    "        # Capture raw output by running command directly\n",
    "        bandit_cmd = f\"bandit -r {file_path} -ll -iii\"\n",
    "        print(f\"Command: {bandit_cmd}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # First get the raw output\n",
    "        raw_output = subprocess.run(\n",
    "            bandit_cmd, \n",
    "            shell=True, \n",
    "            stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.PIPE, \n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        # Then run the benchmark function\n",
    "        success, bandit_result, tool_name, scan_time = test_vulnerabilities_benchmark(file_path)\n",
    "        \n",
    "        print(f\"BANDIT RAW OUTPUT (took {time.time() - start_time:.3f}s):\")\n",
    "        print(\"-\" * 30)\n",
    "        print(raw_output.stdout)\n",
    "        if raw_output.stderr:\n",
    "            print(\"STDERR:\")\n",
    "            print(raw_output.stderr)\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        print(f\"BANDIT PARSED RESULT:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Success: {success}\")\n",
    "        print(f\"Bandit result (True=vulnerable, False=safe): {bandit_result}\")\n",
    "        print(f\"Tool name: {tool_name}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        expected_result = \"vulnerable\" in file_path.lower()\n",
    "        print(f\"EXPECTED RESULT: {expected_result}\")\n",
    "\n",
    "        if success:\n",
    "            match = \"✓ CORRECT\" if bandit_result == expected_result else \"✗ INCORRECT\"\n",
    "            print(f\"MATCH: {match}\")\n",
    "        else:\n",
    "            print(\"MATCH: ✗ FAILED TO ANALYZE\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "\n",
    "# Let's test with both a vulnerable and safe file\n",
    "print(\"Running Bandit debug examples...\")\n",
    "\n",
    "# Try to find a vulnerable file\n",
    "vulnerable_file = None\n",
    "if os.path.exists(\"train/vulnerable\"):\n",
    "    vulnerable_files = list(Path(\"train/vulnerable\").glob(\"**/*.py\"))\n",
    "    if vulnerable_files:\n",
    "        vulnerable_file = str(vulnerable_files[0])\n",
    "        print(f\"\\nTesting with vulnerable file: {vulnerable_file}\")\n",
    "        debug_bandit_example(vulnerable_file)\n",
    "\n",
    "# Try to find a safe file\n",
    "safe_file = None\n",
    "if os.path.exists(\"train/patched\"):\n",
    "    safe_files = list(Path(\"train/patched\").glob(\"**/*.py\"))\n",
    "    if safe_files:\n",
    "        safe_file = str(safe_files[0])\n",
    "        print(f\"\\nTesting with safe file: {safe_file}\")\n",
    "        debug_bandit_example(safe_file)\n",
    "\n",
    "# If we couldn't find test files from the train directories\n",
    "if not vulnerable_file and not safe_file:\n",
    "    print(\"\\nFalling back to a simple test...\")\n",
    "    debug_bandit_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
